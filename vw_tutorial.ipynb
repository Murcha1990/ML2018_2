{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейные методы. Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vowpal Wabbit on GitHub: https://github.com/JohnLangford/vowpal_wabbit\n",
    "\n",
    "Vowpal Wabbit Tutorial: https://github.com/JohnLangford/vowpal_wabbit/wiki/Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = 'train-sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140272\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(train_path)\n",
    "data.head()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open\n",
      "i have two dimensions, first (width, height) and second(width1, height1). how can i retrieve a Point(x,y) from dimensions???\n"
     ]
    }
   ],
   "source": [
    "print(data.OpenStatus[10])\n",
    "print(data.BodyMarkdown[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data.iloc[:50000, :]\n",
    "data_test = data.iloc[70000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def save_to_vw(data, fname):\n",
    "    with open(fname, 'w') as fout:\n",
    "        for _, row in data.iterrows():\n",
    "            text = filter(lambda x: len(x) > 1, re.split(\"[^a-z]\",\n",
    "                                    row.BodyMarkdown.lower()))\n",
    "            text = ' '.join(text)\n",
    "            if row.OpenStatus == \"open\":\n",
    "                target = 1\n",
    "            else:\n",
    "                target = -1\n",
    "            fout.write('{0} |n 0:{1} {2} |t {3}\\n'.format(target, \n",
    "                                        row.ReputationAtPostCreation,\n",
    "                                        row.Tag1,\n",
    "                                        text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_vw(data_train, 'train.vw')\n",
    "save_to_vw(data_test, 'test.vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 |n 0:1 mongodb |t am building corpus of indexed sentences in different languages have collection of languages which have both an objectid and the iso code as key is it better to use reference to the language collection or store key like en or fr suppose it compromise between ease of referencing the language object in that collection speed in doing queries where the sentence has certain language the size of the data on disk any best practices that should know of\r\n",
      "1 |n 0:192 dom |t create xml document with jaxp and search way to insert the schemalocation at the moment my application produces xml version encoding utf root root but need xml version encoding utf root xmlns namespaceurl xmlns xs http www org xmlschema instance xs schemalocation namespaceurl pathtomyschema xsd root my code streamresult result new streamresult writer document doc getdocument transformer trans transfac newtransformer trans setoutputproperty outputkeys indent yes trans setoutputproperty outputkeys method xml trans setoutputproperty outputkeys version trans setoutputproperty outputkeys encoding utf domsource source new domsource depl getaselement doc trans transform source result thanks for your time kasten\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 train.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model.vw\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = train.vw.cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000       81\n",
      "0.500000 0.000000            2            2.0   1.0000   1.0000       98\n",
      "0.906326 1.312652            4            4.0  -1.0000   0.4266      110\n",
      "0.821724 0.737121            8            8.0  -1.0000   0.4654       38\n",
      "0.875065 0.928406           16           16.0   1.0000   0.7270      136\n",
      "0.906826 0.938586           32           32.0   1.0000   0.5103       93\n",
      "0.996030 1.085234           64           64.0   1.0000   0.7147       71\n",
      "0.977266 0.958502          128          128.0  -1.0000  -0.3218       24\n",
      "0.990424 1.003582          256          256.0  -1.0000   0.2728       61\n",
      "0.969661 0.948898          512          512.0   1.0000  -0.3303       33\n",
      "0.939473 0.909286         1024         1024.0   1.0000  -0.2806      189\n",
      "0.933614 0.927754         2048         2048.0  -1.0000  -0.4861       24\n",
      "0.915891 0.898169         4096         4096.0  -1.0000   0.6062       38\n",
      "0.900047 0.884203         8192         8192.0  -1.0000  -0.3484       16\n",
      "0.849804 0.799560        16384        16384.0   1.0000   1.0000      296\n",
      "0.822175 0.794547        32768        32768.0  -1.0000  -1.0000      101\n",
      "0.809638 0.809638        65536        65536.0   1.0000   1.0000      176 h\n",
      "0.808080 0.806521       131072       131072.0  -1.0000  -0.5021       10 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 45000\n",
      "passes used = 5\n",
      "weighted example sum = 225000.000000\n",
      "weighted label sum = 180.000000\n",
      "average loss = 0.799768 h\n",
      "best constant = 0.000800\n",
      "best constant's loss = 0.999999\n",
      "total feature number = 24653095\n"
     ]
    }
   ],
   "source": [
    "!vw -d train.vw -c -k -f model.vw --passes 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for t namespaces.\n",
      "only testing\n",
      "predictions = pred.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000      370\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000      248\n",
      "1.010778 2.021556            4            4.0  -1.0000   1.0000     1028\n",
      "0.660964 0.311150            8            8.0   1.0000   1.0000      230\n",
      "0.535013 0.409062           16           16.0   1.0000   0.5860      306\n",
      "0.523252 0.511491           32           32.0   1.0000  -0.0752       92\n",
      "0.690913 0.858574           64           64.0  -1.0000   0.3738      448\n",
      "0.662274 0.633634          128          128.0   1.0000   0.2049      168\n",
      "0.702460 0.742647          256          256.0  -1.0000   0.0076      162\n",
      "0.721151 0.739842          512          512.0   1.0000   1.0000      202\n",
      "0.731540 0.741928         1024         1024.0  -1.0000  -0.4001       44\n",
      "0.713196 0.694852         2048         2048.0  -1.0000  -0.4640      308\n",
      "0.723218 0.733240         4096         4096.0   1.0000   0.9354      330\n",
      "0.723556 0.723894         8192         8192.0  -1.0000  -0.1248       86\n",
      "0.724842 0.726128        16384        16384.0  -1.0000  -0.1427      158\n",
      "0.727341 0.729841        32768        32768.0  -1.0000   0.2542      128\n",
      "0.727127 0.726912        65536        65536.0   1.0000   1.0000      140\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 70272\n",
      "passes used = 1\n",
      "weighted example sum = 70272.000000\n",
      "weighted label sum = -94.000000\n",
      "average loss = 0.727744\n",
      "best constant = -0.001338\n",
      "best constant's loss = 0.999998\n",
      "total feature number = 15114088\n"
     ]
    }
   ],
   "source": [
    "!vw -d test.vw -i model.vw -t -p pred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n",
      "1\r\n",
      "-1\r\n",
      "1\r\n",
      "0.082040\r\n",
      "1\r\n",
      "-1\r\n",
      "0.805499\r\n",
      "-0.150867\r\n",
      "-0.643998\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 pred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784259407516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calc_vw_qual():\n",
    "    preds = pd.read_csv('pred_tutorial.txt', header=None).iloc[:, 0].values\n",
    "    target = data_test.OpenStatus.values\n",
    "    T = []\n",
    "    for t in target:\n",
    "        if t == 'open':\n",
    "            T.append(1.)\n",
    "        else:\n",
    "            T.append(-1.)\n",
    "    print(roc_auc_score(T, preds))\n",
    "    \n",
    "calc_vw_qual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model.vw\n",
      "Num weight bits = 18\n",
      "learning rate = 0.1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = train.vw.cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000       81\n",
      "0.500000 0.000000            2            2.0   1.0000   1.0000       98\n",
      "0.812846 1.125692            4            4.0  -1.0000   0.2230      110\n",
      "0.769143 0.725440            8            8.0  -1.0000   0.2957       38\n",
      "0.845550 0.921957           16           16.0   1.0000   0.4906      136\n",
      "0.812974 0.780397           32           32.0   1.0000   0.5922       93\n",
      "0.899824 0.986675           64           64.0   1.0000   0.3493       71\n",
      "0.894960 0.890097          128          128.0  -1.0000  -0.0987       24\n",
      "0.893272 0.891584          256          256.0  -1.0000   0.0260       61\n",
      "0.917625 0.941978          512          512.0   1.0000  -0.2435       33\n",
      "0.897855 0.878085         1024         1024.0   1.0000  -0.1547      189\n",
      "0.879552 0.861248         2048         2048.0  -1.0000  -0.2846       24\n",
      "0.856456 0.833361         4096         4096.0  -1.0000   0.2378       38\n",
      "0.839280 0.822103         8192         8192.0  -1.0000  -0.1512       16\n",
      "0.806363 0.773447        16384        16384.0   1.0000   1.0000      296\n",
      "0.783823 0.761283        32768        32768.0  -1.0000  -1.0000      101\n",
      "0.767486 0.767486        65536        65536.0   1.0000   1.0000      176 h\n",
      "0.759695 0.751905       131072       131072.0  -1.0000  -0.3393       10 h\n",
      "0.751583 0.743471       262144       262144.0   1.0000   1.0000      352 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 45000\n",
      "passes used = 7\n",
      "weighted example sum = 315000.000000\n",
      "weighted label sum = 252.000000\n",
      "average loss = 0.744279 h\n",
      "best constant = 0.000800\n",
      "best constant's loss = 0.999999\n",
      "total feature number = 34514333\n",
      "only testing\n",
      "predictions = pred.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000      187\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000      126\n",
      "1.000612 2.001223            4            4.0  -1.0000   1.0000      516\n",
      "0.650465 0.300319            8            8.0   1.0000   1.0000      117\n",
      "0.649761 0.649056           16           16.0   1.0000   0.1935      155\n",
      "0.556570 0.463378           32           32.0   1.0000   0.0324       48\n",
      "0.704121 0.851672           64           64.0  -1.0000   0.1636      226\n",
      "0.692764 0.681407          128          128.0   1.0000   0.2447       86\n",
      "0.717323 0.741882          256          256.0  -1.0000  -0.0195       83\n",
      "0.756079 0.794836          512          512.0   1.0000   0.8771      103\n",
      "0.765657 0.775235         1024         1024.0  -1.0000  -0.4384       24\n",
      "0.748060 0.730462         2048         2048.0  -1.0000  -0.8658      156\n",
      "0.745716 0.743373         4096         4096.0   1.0000   1.0000      167\n",
      "0.751569 0.757421         8192         8192.0  -1.0000  -0.1546       45\n",
      "0.750412 0.749256        16384        16384.0  -1.0000  -0.2153       81\n",
      "0.750514 0.750615        32768        32768.0  -1.0000   0.3694       66\n",
      "0.749129 0.747744        65536        65536.0   1.0000   1.0000       72\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 70272\n",
      "passes used = 1\n",
      "weighted example sum = 70272.000000\n",
      "weighted label sum = -94.000000\n",
      "average loss = 0.749844\n",
      "best constant = -0.001338\n",
      "best constant's loss = 0.999998\n",
      "total feature number = 7697580\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.798460933777\n"
     ]
    }
   ],
   "source": [
    "!vw -d train.vw -c -k -f model.vw --passes 10 -l 0.1\n",
    "!vw -d test.vw -i model.vw -t -p pred.txt\n",
    "print('\\n\\n\\n')\n",
    "calc_vw_qual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-граммы (n=2) - индикаторы того, что два слова встретились рядом. Из \"мама мыла раму\" получаем биграммы \"мама мыла\" и \"мыла раму\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for t namespaces.\n",
      "final_regressor = model.vw\n",
      "Num weight bits = 18\n",
      "learning rate = 0.1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = train.vw.cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000      158\n",
      "0.501991 0.003982            2            2.0   1.0000   0.9369      192\n",
      "0.769028 1.036065            4            4.0  -1.0000   0.1032      216\n",
      "0.788365 0.807701            8            8.0  -1.0000   0.2224       72\n",
      "0.853561 0.918757           16           16.0   1.0000   0.3748      268\n",
      "0.815435 0.777309           32           32.0   1.0000   0.5281      182\n",
      "0.899736 0.984037           64           64.0   1.0000   0.1909      138\n",
      "0.899076 0.898416          128          128.0  -1.0000  -0.0599       44\n",
      "0.900915 0.902754          256          256.0  -1.0000   0.0426      118\n",
      "0.919717 0.938519          512          512.0   1.0000  -0.1645       62\n",
      "0.894198 0.868679         1024         1024.0   1.0000  -0.0957      374\n",
      "0.874036 0.853875         2048         2048.0  -1.0000  -0.2335       44\n",
      "0.848005 0.821974         4096         4096.0  -1.0000   0.1117       72\n",
      "0.832330 0.816654         8192         8192.0  -1.0000  -0.0422       28\n",
      "0.799748 0.767167        16384        16384.0   1.0000   1.0000      588\n",
      "0.771801 0.743853        32768        32768.0  -1.0000  -1.0000      198\n",
      "0.758018 0.758018        65536        65536.0   1.0000   1.0000      348 h\n",
      "0.748931 0.739846       131072       131072.0  -1.0000  -0.3381       16 h\n",
      "0.742015 0.735099       262144       262144.0   1.0000   1.0000      700 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 45000\n",
      "passes used = 7\n",
      "weighted example sum = 315000.000000\n",
      "weighted label sum = 252.000000\n",
      "average loss = 0.735132 h\n",
      "best constant = 0.000800\n",
      "best constant's loss = 0.999999\n",
      "total feature number = 67768757\n",
      "Generating 2-grams for t namespaces.\n",
      "only testing\n",
      "predictions = pred.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000      370\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000      248\n",
      "1.000415 2.000830            4            4.0  -1.0000   1.0000     1028\n",
      "0.726777 0.453139            8            8.0   1.0000   1.0000      230\n",
      "0.611767 0.496757           16           16.0   1.0000   0.9884      306\n",
      "0.530342 0.448916           32           32.0   1.0000   0.0267       92\n",
      "0.677193 0.824044           64           64.0  -1.0000  -0.1322      448\n",
      "0.679815 0.682438          128          128.0   1.0000   0.3016      168\n",
      "0.680352 0.680888          256          256.0  -1.0000   0.2984      162\n",
      "0.712041 0.743731          512          512.0   1.0000   1.0000      202\n",
      "0.728467 0.744893         1024         1024.0  -1.0000  -0.4186       44\n",
      "0.714584 0.700701         2048         2048.0  -1.0000  -0.6617      308\n",
      "0.720519 0.726454         4096         4096.0   1.0000   0.9439      330\n",
      "0.728133 0.735746         8192         8192.0  -1.0000  -0.1757       86\n",
      "0.733176 0.738219        16384        16384.0  -1.0000  -0.0826      158\n",
      "0.736418 0.739660        32768        32768.0  -1.0000   0.2509      128\n",
      "0.735183 0.733949        65536        65536.0   1.0000   1.0000      140\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 70272\n",
      "passes used = 1\n",
      "weighted example sum = 70272.000000\n",
      "weighted label sum = -94.000000\n",
      "average loss = 0.736167\n",
      "best constant = -0.001338\n",
      "best constant's loss = 0.999998\n",
      "total feature number = 15114088\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.804926941366\n"
     ]
    }
   ],
   "source": [
    "!vw -d train.vw -c -k -f model.vw --passes 10 -l 0.1 --ngram t2\n",
    "!vw -d test.vw -i model.vw -t -p pred.txt\n",
    "print('\\n\\n\\n')\n",
    "calc_vw_qual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-skip-n-граммы - как n-граммы, но разрешаем словам быть отдаленными друг от друга не больше, чем на k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for t namespaces.\n",
      "Generating 2-skips for t namespaces.\n",
      "final_regressor = model.vw\n",
      "Num weight bits = 18\n",
      "learning rate = 0.1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = train.vw.cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000      309\n",
      "0.598903 0.197807            2            2.0   1.0000   0.5552      377\n",
      "0.813193 1.027482            4            4.0  -1.0000   0.0751      425\n",
      "0.827788 0.842383            8            8.0  -1.0000   0.1551      137\n",
      "0.878166 0.928544           16           16.0   1.0000   0.2804      529\n",
      "0.833454 0.788743           32           32.0   1.0000   0.4647      357\n",
      "0.919097 1.004739           64           64.0   1.0000   0.1906      269\n",
      "0.925038 0.930980          128          128.0  -1.0000  -0.0388       81\n",
      "0.922979 0.920921          256          256.0  -1.0000  -0.0007      229\n",
      "0.931385 0.939790          512          512.0   1.0000  -0.1543      117\n",
      "0.903010 0.874635         1024         1024.0   1.0000  -0.1395      741\n",
      "0.888596 0.874181         2048         2048.0  -1.0000  -0.2036       81\n",
      "0.858703 0.828809         4096         4096.0  -1.0000   0.0102      137\n",
      "0.841166 0.823629         8192         8192.0  -1.0000  -0.0737       49\n",
      "0.809038 0.776910        16384        16384.0   1.0000   1.0000     1169\n",
      "0.778903 0.748767        32768        32768.0  -1.0000  -1.0000      389\n",
      "0.765944 0.765944        65536        65536.0   1.0000   1.0000      689 h\n",
      "0.755038 0.744134       131072       131072.0  -1.0000  -0.3035       25 h\n",
      "0.747564 0.740090       262144       262144.0   1.0000   1.0000     1393 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 45000\n",
      "passes used = 6\n",
      "weighted example sum = 270000.000000\n",
      "weighted label sum = 216.000000\n",
      "average loss = 0.739264 h\n",
      "best constant = 0.000800\n",
      "best constant's loss = 0.999999\n",
      "total feature number = 114285174\n",
      "Generating 2-grams for t namespaces.\n",
      "Generating 2-skips for t namespaces.\n",
      "only testing\n",
      "predictions = pred.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000      733\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000      489\n",
      "1.000493 2.000987            4            4.0  -1.0000   1.0000     2049\n",
      "0.657113 0.313733            8            8.0   1.0000   1.0000      453\n",
      "0.500792 0.344470           16           16.0   1.0000   0.9267      605\n",
      "0.506029 0.511267           32           32.0   1.0000  -0.0084      177\n",
      "0.681003 0.855977           64           64.0  -1.0000   0.5670      889\n",
      "0.664308 0.647613          128          128.0   1.0000   0.1887      329\n",
      "0.681362 0.698417          256          256.0  -1.0000   0.2793      317\n",
      "0.714238 0.747114          512          512.0   1.0000   1.0000      397\n",
      "0.724605 0.734972         1024         1024.0  -1.0000  -0.3687       81\n",
      "0.719643 0.714680         2048         2048.0  -1.0000  -0.7586      609\n",
      "0.725630 0.731618         4096         4096.0   1.0000   0.7505      653\n",
      "0.732190 0.738751         8192         8192.0  -1.0000  -0.1384      165\n",
      "0.736435 0.740679        16384        16384.0  -1.0000  -0.0649      309\n",
      "0.740426 0.744417        32768        32768.0  -1.0000   0.2549      249\n",
      "0.740816 0.741207        65536        65536.0   1.0000   1.0000      273\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 70272\n",
      "passes used = 1\n",
      "weighted example sum = 70272.000000\n",
      "weighted label sum = -94.000000\n",
      "average loss = 0.741547\n",
      "best constant = -0.001338\n",
      "best constant's loss = 0.999998\n",
      "total feature number = 29736304\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.802041747518\n"
     ]
    }
   ],
   "source": [
    "!vw -d train.vw -c -k -f model.vw --passes 10 -l 0.1 --ngram t2 --skips t2\n",
    "!vw -d test.vw -i model.vw -t -p pred.txt\n",
    "print('\\n\\n\\n')\n",
    "calc_vw_qual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for t namespaces.\n",
      "final_regressor = model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = train.vw.cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000      158\n",
      "0.501991 0.003982            2            2.0   1.0000   0.9369      192\n",
      "0.768823 1.035654            4            4.0  -1.0000   0.1028      216\n",
      "0.788250 0.807676            8            8.0  -1.0000   0.2224       72\n",
      "0.852059 0.915868           16           16.0   1.0000   0.3741      268\n",
      "0.814879 0.777699           32           32.0   1.0000   0.5283      182\n",
      "0.897638 0.980397           64           64.0   1.0000   0.2017      138\n",
      "0.897445 0.897252          128          128.0  -1.0000  -0.0599       44\n",
      "0.898531 0.899617          256          256.0  -1.0000   0.0460      118\n",
      "0.919904 0.941277          512          512.0   1.0000  -0.1587       62\n",
      "0.893383 0.866862         1024         1024.0   1.0000  -0.1440      374\n",
      "0.874990 0.856597         2048         2048.0  -1.0000  -0.2360       44\n",
      "0.846852 0.818714         4096         4096.0  -1.0000   0.1488       72\n",
      "0.830443 0.814034         8192         8192.0  -1.0000  -0.0282       28\n",
      "0.796314 0.762186        16384        16384.0   1.0000   1.0000      588\n",
      "0.767529 0.738743        32768        32768.0  -1.0000  -1.0000      198\n",
      "0.752728 0.752728        65536        65536.0   1.0000   1.0000      348 h\n",
      "0.742912 0.733098       131072       131072.0  -1.0000  -0.2960       16 h\n",
      "0.733790 0.724669       262144       262144.0   1.0000   1.0000      700 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 45000\n",
      "passes used = 7\n",
      "weighted example sum = 315000.000000\n",
      "weighted label sum = 252.000000\n",
      "average loss = 0.725734 h\n",
      "best constant = 0.000800\n",
      "best constant's loss = 0.999999\n",
      "total feature number = 67768757\n",
      "Generating 2-grams for t namespaces.\n",
      "only testing\n",
      "predictions = pred.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000      370\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000      248\n",
      "1.010778 2.021556            4            4.0  -1.0000   1.0000     1028\n",
      "0.660964 0.311150            8            8.0   1.0000   1.0000      230\n",
      "0.535013 0.409062           16           16.0   1.0000   0.5860      306\n",
      "0.523252 0.511491           32           32.0   1.0000  -0.0752       92\n",
      "0.690913 0.858574           64           64.0  -1.0000   0.3738      448\n",
      "0.662274 0.633634          128          128.0   1.0000   0.2049      168\n",
      "0.702460 0.742647          256          256.0  -1.0000   0.0076      162\n",
      "0.721151 0.739842          512          512.0   1.0000   1.0000      202\n",
      "0.731540 0.741928         1024         1024.0  -1.0000  -0.4001       44\n",
      "0.713196 0.694852         2048         2048.0  -1.0000  -0.4640      308\n",
      "0.723218 0.733240         4096         4096.0   1.0000   0.9354      330\n",
      "0.723556 0.723894         8192         8192.0  -1.0000  -0.1248       86\n",
      "0.724842 0.726128        16384        16384.0  -1.0000  -0.1427      158\n",
      "0.727341 0.729841        32768        32768.0  -1.0000   0.2542      128\n",
      "0.727127 0.726912        65536        65536.0   1.0000   1.0000      140\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 70272\n",
      "passes used = 1\n",
      "weighted example sum = 70272.000000\n",
      "weighted label sum = -94.000000\n",
      "average loss = 0.727744\n",
      "best constant = -0.001338\n",
      "best constant's loss = 0.999998\n",
      "total feature number = 15114088\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.811472462615\n"
     ]
    }
   ],
   "source": [
    "!vw -d train.vw -c -k -f model.vw --passes 10 -l 0.1 --ngram t2 -b 28\n",
    "!vw -d test.vw -i model.vw -t -p pred.txt\n",
    "print('\\n\\n\\n')\n",
    "calc_vw_qual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
